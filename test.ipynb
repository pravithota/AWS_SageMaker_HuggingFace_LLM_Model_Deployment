{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "# sagemaker session bucket used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket = None\n",
    "\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    sagemaker_session_bucket =sess.default_bucket()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Role management\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "print(f'sagemaker role:{role}')\n",
    "print(f'{sess.boto_region_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "hub = {\n",
    "    'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad', # model id from hf.co/models\n",
    "    'HF_TASK': 'question-answering'  # NLP task you want to use for predictions\n",
    "}\n",
    "\n",
    "# Create huggingface model class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    transformers_version='4.26',\n",
    "    pytorch_version='1.13',\n",
    "    py_version='py310'\n",
    ")\n",
    "\n",
    "# Deploy model to Sagemaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    isinstance_type='m1.m5.xlarge'\n",
    ")\n",
    "\n",
    "# example request: you always need to define 'inputs'\n",
    "data = {\n",
    "    'inputs': {\n",
    "        'question': 'What I used to teach?',\n",
    "        'context': 'My Name is kK and I live in Bangalore. I used to teach data science.'\n",
    "    }\n",
    "}\n",
    "#request\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer: {'data sciecne'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compared to deploying regular Hugging Face Models, We first need to retireve the container uri and provide it to our Hugging Face Model class\n",
    "with a image_uri pointing to the image. To retrieve the new Hugging Face LLM Deep Learning Container in Amazon Sagemaker, We can use the get_huggingface_llm_image_uri method provided by the SageMaker SDK. This method allows us to retrieve the URI for the desired Hugging Face LLM DLC based on the specified backend, session, region and version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import get_huggingface_llm_image_uri\n",
    "\n",
    "# Retrieve the llm image uri\n",
    "llm_image = get_huggingface_llm_image_uri(\n",
    "    'huggingface',\n",
    "    version='0.8.2'\n",
    ")\n",
    "\n",
    "print(f'llm image uri:{llm_image}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sagemaker.huggingface import HuggingFaceModel\n",
    "\n",
    "# saegmaker config\n",
    "isinstance_type = 'ml.g5.12xlarge'\n",
    "number_of_gpu = 4\n",
    "\n",
    "# TGI config\n",
    "config = {\n",
    "    'HF_MODEL_ID': 'tiiuae/falcon-40b-instruct', # model id from hf.co/models\n",
    "    'SM_NUM_GPUS': json.dumps(number_of_gpu), # Number of GPU used per replica\n",
    "    'MAX_INPUT_LENGTH': json.dumps(1024), # Max length of input text\n",
    "    'MAX_TOTAL_TOKENS': json.dumps(2048), # Max length of the generation (including input text)\n",
    "    #'HF_MODEL_QUANTIZE': 'bitsandbytes' # comment in to qunatize\n",
    "}\n",
    "\n",
    "# create HuggingFaceModel\n",
    "llm_model = HuggingFaceModel(\n",
    "    role=role,\n",
    "    image_uri=llm_image,\n",
    "    env = config\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define payload\n",
    "prompt = '''\n",
    "You are helpful assistant, called Falcon. Knowing everything about AWS.\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Falcon:\n",
    "'''\n",
    "# hyperparameters for llm\n",
    "payload = {\n",
    "    'inputs': prompt,\n",
    "    'parameters': {\n",
    "        'do_sample': True,\n",
    "        'top_p': 0.9,\n",
    "        'temperature': 0.8,\n",
    "        'max_new_tokens': 1024,\n",
    "        'repetition_penalty': 1.03,\n",
    "        'stop': ['\\nUser:','<|endoftext|>','</s>']\n",
    "    }\n",
    "}\n",
    "# send request to endpoint\n",
    "response = llm_model.prompt(payload)\n",
    "\n",
    "for seq in response:\n",
    "    print(f'Result: {seq['generated_text']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chicken",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
