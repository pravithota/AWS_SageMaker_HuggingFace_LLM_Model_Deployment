{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\DataScience\\LLMs\\AWS-SageMaker-LLMS-Deployment-Demo\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer,AutoModelForSeq2SeqLM\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "import base64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint='MBZUAI/LaMini-T5-738M'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer=AutoTokenizer.from_pretrained(checkpoint)\n",
    "base_model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint, torch_dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm_pipeline():\n",
    "    pipe=pipeline(\n",
    "        'text2text-generation',\n",
    "        model=base_model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_length=256,\n",
    "        do_sample=True,\n",
    "        temperature=0.3,\n",
    "        top_p=0.95\n",
    "    )\n",
    "    local_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "    return local_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_prompt = 'Write an article on Artificial Intelligence'\n",
    "model = llm_pipeline()\n",
    "generated_text = model(input_prompt)\n",
    "generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sagemaker\n",
    "import boto3\n",
    "from sagemaker.huggingface import HuggingFaceModel, get_huggingface_llm_image_uri\n",
    "\n",
    "try:\n",
    "    role = sagemaker.get_execution_role()\n",
    "except ValueError:\n",
    "    iam = boto3.client('iam')\n",
    "    role = iam.get_role(RoleName='sagemaker_execution_role')['Role']['Arn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface.model import HuggingFaceModel\n",
    "\n",
    "hub = {\n",
    "    'HF_MODEL_ID':checkpoint, # model id from hf.co/models\n",
    "    'HF_TASK': 'text2text-generation',  # NLP task you want to use for predictions\n",
    "    'torch_dtype':'torch.float32'\n",
    "}\n",
    "\n",
    "# Create huggingface model class\n",
    "huggingface_model = HuggingFaceModel(\n",
    "    env=hub,\n",
    "    role=role,\n",
    "    transformers_version='4.26',\n",
    "    pytorch_version='1.13',\n",
    "    py_version='py310',\n",
    "    image_uri=get_huggingface_llm_image_uri('huggingface', version='0.8.2')\n",
    ")\n",
    "\n",
    "# Deploy model to Sagemaker Inference\n",
    "predictor = huggingface_model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    isinstance_type='m1.m5.xlarge',\n",
    "    container_startup_health_check_timeout=300\n",
    ")\n",
    "\n",
    "# example request: you always need to define 'inputs'\n",
    "data = {\n",
    "    'inputs': {\n",
    "        'question': 'What I used to teach?',\n",
    "        'context': 'My Name is kK and I live in Bangalore. I used to teach data science.'\n",
    "    }\n",
    "}\n",
    "#request\n",
    "predictor.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define payload\n",
    "prompt = '''\n",
    "You are helpful assistant, called Falcon. Knowing everything about AWS.\n",
    "User: Can you tell me something about Amazon SageMaker?\n",
    "Falcon:\n",
    "'''\n",
    "# hyperparameters for llm\n",
    "payload = {\n",
    "    'inputs': prompt,\n",
    "    'parameters': {\n",
    "        'do_sample': True,\n",
    "        'top_p': 0.9,\n",
    "        'temperature': 0.8,\n",
    "        'max_new_tokens': 1024,\n",
    "        'repetition_penalty': 1.03,\n",
    "        'stop': ['\\nUser:','<|endoftext|>','']\n",
    "    }\n",
    "}\n",
    "# send request to endpoint\n",
    "response = huggingface_model.prompt(payload)\n",
    "\n",
    "for seq in response:\n",
    "    print(f'Result: {seq['generated_text']}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint='huggingface-python-tgi-inference-2023-07-01-14-10-51-753'\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runtime = boto3.client('runtime.sagemaker')\n",
    "response=runtime.invoke_endpoint(EndpointName=endpoint, ContentType='application/json', Body=json.dumps(payload))\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = json.loads(response['Body'].read().decode('utf-8'))\n",
    "prediction\n",
    "prediction[0]['generated_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " ## Lambda function\n",
    "\n",
    "import json\n",
    "import boto3\n",
    "\n",
    "ENDPOINT = 'huggingface-python-tgi-inference-2023-07-01-14-10-51-753'\n",
    "runtime = boto3.clinet('runtime.sagemaker')\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    query_params = event['queryStringParameters']\n",
    "    query = query_params.get('query')\n",
    "    \n",
    "    payload = {\n",
    "    'inputs': query,\n",
    "    'parameters': {\n",
    "        'do_sample': True,\n",
    "        'top_p': 0.9,\n",
    "        'temperature': 0.8,\n",
    "        'max_new_tokens': 1024,\n",
    "        'repetition_penalty': 1.03    \n",
    "    }\n",
    "    }\n",
    "\n",
    "    response=runtime.invoke_endpoint(EndpointName=endpoint, ContentType='application/json', Body=json.dumps(payload))\n",
    "    prediction = json.loads(response['Body'].read().decode('utf-8'))\n",
    "\n",
    "    final_result = prediction[0]['generated_text']\n",
    "    \n",
    "    return {\n",
    "        'statuscode': 200,\n",
    "        'body': json.dumps(final_result)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chicken",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
